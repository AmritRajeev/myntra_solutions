{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import copy\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms , models\n",
    "\n",
    "# Pennylane\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# OpenMP: number of parallel threads.\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qubits = 4                # Number of qubits\n",
    "step = 0.0004               # Learning rate\n",
    "batch_size = 4              # Number of samples for each training step\n",
    "num_epochs = 1              # Number of training epochs\n",
    "q_depth = 6                 # Depth of the quantum circuit (number of variational layers)\n",
    "gamma_lr_scheduler = 0.1    # Learning rate reduction applied every 10 epochs.\n",
    "q_delta = 0.01              # Initial spread of random quantum weights\n",
    "start_time = time.time()    # Start of the computation timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = qml.device(\"default.qubit\", wires=n_qubits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"/kaggle/input/fashion-product-images-dataset/fashion-dataset/fashion-dataset/\"\n",
    "print(os.listdir(DATASET_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the rows and dropping the ones with errors\n",
    "df = pd.read_csv(DATASET_PATH + \"styles.csv\", nrows=44416, error_bad_lines=False)\n",
    "df['image'] = df.apply(lambda row: str(row['id']) + \".jpg\", axis=1)\n",
    "df = df.reset_index(drop=True)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_Pictures = 500\n",
    "N_Classes = np.sum(df.articleType.value_counts().to_numpy() > N_Pictures)\n",
    "#Number of classes with sufficient images to train on:\n",
    "N_Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspecting the item classes that made it to our new dataset\n",
    "temp = df.articleType.value_counts().sort_values(ascending=False)[:N_Classes]\n",
    "temp[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving item types(labels) with their counts\n",
    "items_count = temp.values\n",
    "items_label = temp.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating new dataframes for training/validation\n",
    "df_train = pd.DataFrame(columns=['articleType','image'])\n",
    "df_val   = pd.DataFrame(columns=['articleType','image'])\n",
    "\n",
    "\n",
    "for ii in range(0,N_Classes):\n",
    "    \n",
    "    #print(items_label[ii])\n",
    "    \n",
    "    temp = df[df.articleType==items_label[ii]].sample(N_Pictures)\n",
    "\n",
    "    df_train = pd.concat([df_train, temp[ :int(N_Pictures*0.6) ][['articleType','image']] ]            , sort=False)\n",
    "    df_val   = pd.concat([df_val,   temp[  int(N_Pictures*0.6): N_Pictures ][['articleType','image']] ], sort=False)\n",
    "\n",
    "df_train.reset_index(drop=True)\n",
    "df_val.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    \"train\": transforms.Compose(\n",
    "        [\n",
    "            # transforms.RandomResizedCrop(224),     # uncomment for data augmentation\n",
    "            # transforms.RandomHorizontalFlip(),     # uncomment for data augmentation\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            # Normalize input channels using mean values and standard deviations of ImageNet.\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    ),\n",
    "    \"val\": transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    ),\n",
    "}\n",
    "\n",
    "data_dir = 'data'\n",
    "image_datasets = {\n",
    "        x if x == \"train\" else \"validation\": datasets.ImageFolder(\n",
    "            os.path.join(data_dir, x), data_transforms[x]\n",
    "        )\n",
    "        for x in [\"train\", \"val\"]\n",
    "    }\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in [\"train\", \"validation\"]}\n",
    "class_names = image_datasets[\"train\"].classes\n",
    "\n",
    "    # Initialize dataloader\n",
    "dataloaders = {\n",
    "        x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True)\n",
    "        for x in [\"train\", \"validation\"]\n",
    "    }\n",
    "\n",
    "# function to plot images\n",
    "def imshow(inp, title=None):\n",
    "    \"\"\"Display image from tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    # Inverse of the initial normalization operation.\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create folders for new dataset\n",
    "os.chdir(r'/kaggle/working/')\n",
    "os.mkdir('data')\n",
    "os.mkdir('data/train')\n",
    "os.mkdir('data/val')\n",
    "os.chdir(r'/kaggle/working/')\n",
    "\n",
    "data = {'train': df_train, 'val': df_val}\n",
    "\n",
    "# and save each individual image to the new directory\n",
    "for x in ['train','val']:\n",
    "    \n",
    "    print(x)\n",
    "    \n",
    "    for label, file in data[x].values:\n",
    "        \n",
    "        try:\n",
    "            img = Image.open(DATASET_PATH+'images/'+file)\n",
    "        except FileNotFoundError:\n",
    "            # If file does not exist continue\n",
    "            continue\n",
    "            \n",
    "        #Else save file to new directory  \n",
    "        try:\n",
    "            img.save('data/'+x+'/'+label+'/'+file) \n",
    "\n",
    "        except FileNotFoundError:\n",
    "            #If folder does not exist, create one and save the image\n",
    "            os.mkdir('data/'+x+'/'+label)\n",
    "            img.save('data/'+x+'/'+label+'/'+file)\n",
    "            print(label,end=' ')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "\n",
    "# Get a batch of training data\n",
    "inputs, classes = next(iter(dataloaders['train']))\n",
    "\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "imshow(out, title=[class_names[x] for x in classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def H_layer(nqubits):\n",
    "    \"\"\"Layer of single-qubit Hadamard gates.\n",
    "    \"\"\"\n",
    "    for idx in range(nqubits):\n",
    "        qml.Hadamard(wires=idx)\n",
    "\n",
    "\n",
    "def RY_layer(w):\n",
    "    \"\"\"Layer of parametrized qubit rotations around the y axis.\n",
    "    \"\"\"\n",
    "    for idx, element in enumerate(w):\n",
    "        qml.RY(element, wires=idx)\n",
    "\n",
    "\n",
    "def entangling_layer(nqubits):\n",
    "    \"\"\"Layer of CNOTs followed by another shifted layer of CNOT.\n",
    "    \"\"\"\n",
    "    # In other words it should apply something like :\n",
    "    # CNOT  CNOT  CNOT  CNOT...  CNOT\n",
    "    #   CNOT  CNOT  CNOT...  CNOT\n",
    "    for i in range(0, nqubits - 1, 2):  # Loop over even indices: i=0,2,...N-2\n",
    "        qml.CNOT(wires=[i, i + 1])\n",
    "    for i in range(1, nqubits - 1, 2):  # Loop over odd indices:  i=1,3,...N-3\n",
    "        qml.CNOT(wires=[i, i + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@qml.qnode(dev, interface=\"torch\")\n",
    "def quantum_net(q_input_features, q_weights_flat):\n",
    "    \"\"\"\n",
    "    The variational quantum circuit.\n",
    "    \"\"\"\n",
    "\n",
    "    # Reshape weights\n",
    "    q_weights = q_weights_flat.reshape(q_depth, n_qubits)\n",
    "\n",
    "    # Start from state |+> , unbiased w.r.t. |0> and |1>\n",
    "    H_layer(n_qubits)\n",
    "\n",
    "    # Embed features in the quantum node\n",
    "    RY_layer(q_input_features)\n",
    "\n",
    "    # Sequence of trainable variational layers\n",
    "    for k in range(q_depth):\n",
    "        entangling_layer(n_qubits)\n",
    "        RY_layer(q_weights[k])\n",
    "\n",
    "    # Expectation values in the Z basis\n",
    "    exp_vals = [qml.expval(qml.PauliZ(position)) for position in range(n_qubits)]\n",
    "    return tuple(exp_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DressedQuantumNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Torch module implementing the *dressed* quantum net.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Definition of the *dressed* layout.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.pre_net = nn.Linear(512, n_qubits)\n",
    "        self.q_params = nn.Parameter(q_delta * torch.randn(q_depth * n_qubits))\n",
    "        self.post_net = nn.Linear(n_qubits, 2)\n",
    "\n",
    "    def forward(self, input_features):\n",
    "        \"\"\"\n",
    "        Defining how tensors are supposed to move through the *dressed* quantum\n",
    "        net.\n",
    "        \"\"\"\n",
    "\n",
    "        # obtain the input features for the quantum circuit\n",
    "        # by reducing the feature dimension from 512 to 4\n",
    "        pre_out = self.pre_net(input_features)\n",
    "        q_in = torch.tanh(pre_out) * np.pi / 2.0\n",
    "\n",
    "        # Apply the quantum circuit to each element of the batch and append to q_out\n",
    "        q_out = torch.Tensor(0, n_qubits)\n",
    "        q_out = q_out.to(device)\n",
    "        for elem in q_in:\n",
    "            q_out_elem = quantum_net(elem, self.q_params).float().unsqueeze(0)\n",
    "            q_out = torch.cat((q_out, q_out_elem))\n",
    "\n",
    "        # return the two-dimensional prediction from the postprocessing layer\n",
    "        return self.post_net(q_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hybrid = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "for param in model_hybrid.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "# Notice that model_hybrid.fc is the last layer of ResNet18\n",
    "model_hybrid.fc = DressedQuantumNet()\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "\n",
    "#Changing the number of outputs in the last layer to the number of different item types\n",
    "model_ft.fc = nn.Linear(num_ftrs, len(class_names))\n",
    "\n",
    "# Use CUDA or CPU according to the \"device\" object.\n",
    "model_ft = model_hybrid.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os \n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from keras import Model\n",
    "from keras.applications import DenseNet121\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.densenet import preprocess_input, decode_predictions\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.utils import plot_model\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import cv2\n",
    "import pathlib\n",
    "from sklearn.metrics.pairwise import linear_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../input/fashion-product-images-dataset/fashion-dataset/fashion-dataset/'\n",
    "dataset_path = pathlib.Path(path)\n",
    "images=os.listdir(dataset_path)\n",
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "for i in range(10, 20):\n",
    "    plt.subplot(6, 10, i-10+1)\n",
    "    cloth_img =  mpimg.imread(path + 'images/100'+ str(i) +'.jpg')\n",
    "    plt.imshow(cloth_img)\n",
    "    plt.axis(\"off\")\n",
    "plt.subplots_adjust(wspace=-0.5, hspace=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path + \"styles.csv\", nrows=6000, error_bad_lines=False)\n",
    "df['image'] = df.apply(lambda x: str(x['id']) + \".jpg\", axis=1)\n",
    "df = df.reset_index(drop=True)\n",
    "print(df.shape)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image dim\n",
    "img_width, img_height, chnl = 200, 200, 3\n",
    "\n",
    "# DenseNet121\n",
    "densenet = DenseNet121(include_top=False, weights='imagenet', input_shape=(img_width, img_height, chnl))\n",
    "densenet.trainable = False\n",
    "\n",
    "# Add Layer Embedding\n",
    "model = keras.Sequential([\n",
    "    densenet,\n",
    "    MaxPooling2D()\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_path(img):\n",
    "    return path + 'images/' + img\n",
    "def model_predict(model, img_name):\n",
    "    # Reshape\n",
    "    img = image.load_img(img_path(img_name), target_size=(img_width, img_height))\n",
    "    # img to Array\n",
    "    x   = image.img_to_array(img)\n",
    "    # Expand Dim (1, w, h)\n",
    "    x   = np.expand_dims(x, axis=0)\n",
    "    # Pre process Input\n",
    "    x   = preprocess_input(x)\n",
    "    return model.predict(x).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy      = df\n",
    "df_embedding = df_copy['image'].apply(lambda x: model_predict(model, x))\n",
    "df_embedding        = df_embedding.apply(pd.Series)\n",
    "df_embedding.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim = linear_kernel(df_embedding, df_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = pd.Series(range(len(df)), index=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations(index, df, cosine_sim=cosine_sim):\n",
    "    idx = indices[index]\n",
    "\n",
    "    # Get the pairwsie similarity scores of all clothes with that one\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "\n",
    "    # Sort the clothes based on the similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the scores of the 15 most similar clothes\n",
    "    sim_scores = sim_scores[1:15]\n",
    "\n",
    "    # Get the clothes indices\n",
    "    cloth_indices = [i[0] for i in sim_scores]\n",
    "\n",
    "    # Return the top 10 most similar movies\n",
    "    return df['image'].iloc[cloth_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_img_indx = 5\n",
    "\n",
    "recommendation = get_recommendations(chosen_img_indx, df, cosine_sim)\n",
    "recommendation_list = recommendation.to_list()\n",
    "#chosen image\n",
    "chosen_img =  mpimg.imread(path + 'images/' + df.iloc[chosen_img_indx].image)\n",
    "plt.title(\"Chosen image\")\n",
    "plt.imshow(chosen_img)\n",
    "#recommended images\n",
    "plt.figure(figsize=(20,20))\n",
    "j=0\n",
    "for i in recommendation_list:\n",
    "    plt.subplot(6, 10, j+1)\n",
    "    cloth_img =  mpimg.imread(path + 'images/'+ i)\n",
    "    plt.imshow(cloth_img)\n",
    "    plt.axis(\"off\")\n",
    "    j+=1\n",
    "plt.title(\"recommended images\")\n",
    "plt.subplots_adjust(wspace=-0.5, hspace=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_img_indx = 200\n",
    "\n",
    "recommendation = get_recommendations(chosen_img_indx, df, cosine_sim)\n",
    "recommendation_list = recommendation.to_list()\n",
    "#chosen image\n",
    "chosen_img =  mpimg.imread(path + 'images/' + df.iloc[chosen_img_indx].image)\n",
    "plt.title(\"Chosen image\")\n",
    "plt.imshow(chosen_img)\n",
    "#recommended images\n",
    "plt.figure(figsize=(20,20))\n",
    "j=0\n",
    "for i in recommendation_list:\n",
    "    plt.subplot(6, 10, j+1)\n",
    "    cloth_img =  mpimg.imread(path + 'images/'+ i)\n",
    "    plt.imshow(cloth_img)\n",
    "    plt.axis(\"off\")\n",
    "    j+=1\n",
    "plt.title(\"recommended images\")\n",
    "plt.subplots_adjust(wspace=-0.5, hspace=1)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
